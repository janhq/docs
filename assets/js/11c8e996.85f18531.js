"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[2563],{11318:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>a});var i=s(74848),r=s(28453);const t={title:"TensorRT-LLM",slug:"/guides/providers/tensorrt-llm",keywords:["Jan","Customizable Intelligence","LLM","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","TensorRT-LLM Extension","TensorRT","tensorRT","extension"]},l=void 0,o={id:"guides/providers/tensorrt-llm",title:"TensorRT-LLM",description:"TensorRT-LLM support was launched in 0.4.9, and should be regarded as an Experimental feature.",source:"@site/docs/guides/providers/tensorrt-llm.md",sourceDirName:"guides/providers",slug:"/guides/providers/tensorrt-llm",permalink:"/guides/providers/tensorrt-llm",draft:!1,unlisted:!1,editUrl:"https://github.com/janhq/docs/tree/main/docs/guides/providers/tensorrt-llm.md",tags:[],version:"current",lastUpdatedBy:"Henry",lastUpdatedAt:1711732157,formattedLastUpdatedAt:"Mar 29, 2024",frontMatter:{title:"TensorRT-LLM",slug:"/guides/providers/tensorrt-llm",keywords:["Jan","Customizable Intelligence","LLM","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","TensorRT-LLM Extension","TensorRT","tensorRT","extension"]}},d={},a=[{value:"What is TensorRT-LLM?",id:"what-is-tensorrt-llm",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software",id:"software",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Install TensorRT-Extension",id:"install-tensorrt-extension",level:3},{value:"Download a TensorRT-LLM Model",id:"download-a-tensorrt-llm-model",level:3},{value:"Importing Pre-built Models",id:"importing-pre-built-models",level:3},{value:"Using a TensorRT-LLM Model",id:"using-a-tensorrt-llm-model",level:3},{value:"Configure Settings",id:"configure-settings",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Extension Details",id:"extension-details",level:2},{value:"Manual Build",id:"manual-build",level:3},{value:"Uninstall Extension",id:"uninstall-extension",level:3},{value:"Build your own TensorRT models",id:"build-your-own-tensorrt-models",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.p,{children:"TensorRT-LLM support was launched in 0.4.9, and should be regarded as an Experimental feature."}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Only Windows is supported for now."}),"\n",(0,i.jsxs)(n.li,{children:["Please report bugs in our Discord's ",(0,i.jsx)(n.a,{href:"https://discord.com/channels/1107178041848909847/1201832734704795688",children:"#tensorrt-llm"})," channel."]}),"\n"]})]}),"\n",(0,i.jsxs)(n.p,{children:["Jan supports ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"})," as an alternate Inference Engine, for users who have Nvidia GPUs with large VRAM. TensorRT-LLM allows for blazing fast inference, but requires Nvidia GPUs with ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/TensorRT-LLM/memory.html",children:"larger VRAM"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"what-is-tensorrt-llm",children:"What is TensorRT-LLM?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"})," is an hardware-optimized LLM inference engine for Nvidia GPUs, that compiles models to run extremely fast on Nvidia GPUs."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Mainly used on Nvidia's Datacenter-grade GPUs like the H100s ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/TensorRT-LLM/blogs/H100vsA100.html",children:"to produce 10,000 tok/s"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Can be used on Nvidia's workstation (e.g. ",(0,i.jsx)(n.a,{href:"https://www.nvidia.com/en-us/design-visualization/rtx-6000/",children:"A6000"}),") and consumer-grade GPUs (e.g. ",(0,i.jsx)(n.a,{href:"https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4090/",children:"RTX 4090"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{title:"Benefits",type:"tip",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Our performance testing shows 20-40% faster token/s speeds on consumer-grade GPUs"}),"\n",(0,i.jsx)(n.li,{children:"On datacenter-grade GPUs, TensorRT-LLM can go up to 10,000 tokens/s"}),"\n",(0,i.jsxs)(n.li,{children:["TensorRT-LLM is a relatively new library, that was ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM/graphs/contributors",children:"released in Sept 2023"}),". We anticipate performance and resource utilization improvements in the future."]}),"\n"]})}),"\n",(0,i.jsx)(n.admonition,{title:"Caveats",type:"warning",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'TensorRT-LLM requires models to be compiled into GPU and OS-specific "Model Engines" (vs. GGUF\'s "convert once, run anywhere" approach)'}),"\n",(0,i.jsx)(n.li,{children:"TensorRT-LLM Model Engines tend to utilize larger amount of VRAM and RAM in exchange for performance"}),"\n",(0,i.jsx)(n.li,{children:"This usually means only people with top-of-the-line Nvidia GPUs can use TensorRT-LLM"}),"\n"]})}),"\n",(0,i.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Windows PC"}),"\n",(0,i.jsx)(n.li,{children:"Nvidia GPU(s): Ada or Ampere series (i.e. RTX 4000s & 3000s). More will be supported soon."}),"\n",(0,i.jsx)(n.li,{children:"3GB+ of disk space to download TRT-LLM artifacts and a Nitro binary"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Compatible GPUs"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Architecture"}),(0,i.jsx)(n.th,{children:"Supported?"}),(0,i.jsx)(n.th,{children:"Consumer-grade"}),(0,i.jsx)(n.th,{children:"Workstation-grade"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Ada"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"4050 and above"}),(0,i.jsx)(n.td,{children:"RTX A2000 Ada"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Ampere"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"3050 and above"}),(0,i.jsx)(n.td,{children:"A100"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Turing"}),(0,i.jsx)(n.td,{children:"\u274c"}),(0,i.jsx)(n.td,{children:"Not Supported"}),(0,i.jsx)(n.td,{children:"Not Supported"})]})]})]}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["Please ping us in Discord's ",(0,i.jsx)(n.a,{href:"https://discord.com/channels/1107178041848909847/1201832734704795688",children:"#tensorrt-llm"})," channel if you would like Turing support."]})}),"\n",(0,i.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Jan v0.4.9+ or Jan v0.4.8-321+ (nightly)"}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://jan.ai/guides/common-error/not-using-gpu/#1-ensure-gpu-mode-requirements",children:"Nvidia Driver v535+"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://jan.ai/guides/common-error/not-using-gpu/#1-ensure-gpu-mode-requirements",children:"CUDA Toolkit v12.2+"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,i.jsx)(n.h3,{id:"install-tensorrt-extension",children:"Install TensorRT-Extension"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Go to Settings > Extensions"}),"\n",(0,i.jsx)(n.li,{children:"Install the TensorRT-LLM Extension"}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsx)(n.p,{children:"You can check if files have been correctly downloaded:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"ls ~\\jan\\extensions\\@janhq\\tensorrt-llm-extension\\dist\\bin\n# Your Extension Folder should now include `nitro.exe`, among other `.dll` files needed to run TRT-LLM\n"})})]}),"\n",(0,i.jsx)(n.h3,{id:"download-a-tensorrt-llm-model",children:"Download a TensorRT-LLM Model"}),"\n",(0,i.jsxs)(n.p,{children:["Jan's Hub has a few pre-compiled TensorRT-LLM models that you can download, which have a ",(0,i.jsx)(n.code,{children:"TensorRT-LLM"})," label"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"We automatically download the TensorRT-LLM Model Engine for your GPU architecture"}),"\n",(0,i.jsx)(n.li,{children:"We have made a few 1.1b models available that can run even on Laptop GPUs with 8gb VRAM"}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"OS"}),(0,i.jsx)(n.th,{children:"Ada (40XX)"}),(0,i.jsx)(n.th,{children:"Ampere (30XX)"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Llamacorn 1.1b"}),(0,i.jsx)(n.td,{children:"Windows"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"TinyLlama-1.1b, fine-tuned for usability"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"TinyJensen 1.1b"}),(0,i.jsx)(n.td,{children:"Windows"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"TinyLlama-1.1b, fine-tuned on Jensen Huang speeches"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Mistral Instruct 7b"}),(0,i.jsx)(n.td,{children:"Windows"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"\u2705"}),(0,i.jsx)(n.td,{children:"Mistral"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"importing-pre-built-models",children:"Importing Pre-built Models"}),"\n",(0,i.jsxs)(n.p,{children:["You can import a pre-built model, by creating a new folder in Jan's ",(0,i.jsx)(n.code,{children:"/models"})," directory that includes:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["TensorRT-LLM Engine files (e.g. ",(0,i.jsx)(n.code,{children:"tokenizer"}),", ",(0,i.jsx)(n.code,{children:".engine"}),", etc)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model.json"})," that registers these files, and specifies ",(0,i.jsx)(n.code,{children:"engine"})," as ",(0,i.jsx)(n.code,{children:"nitro-tensorrt-llm"})]}),"\n"]}),"\n",(0,i.jsxs)(n.admonition,{title:"Sample model.json",type:"note",children:[(0,i.jsxs)(n.p,{children:["Note the ",(0,i.jsx)(n.code,{children:"engine"})," is ",(0,i.jsx)(n.code,{children:"nitro-tensorrt-llm"}),": this won't work without it!"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-js",children:'{\n  "sources": [\n    {\n      "filename": "config.json",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/config.json"\n    },\n    {\n      "filename": "mistral_float16_tp1_rank0.engine",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/mistral_float16_tp1_rank0.engine"\n    },\n    {\n      "filename": "tokenizer.model",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/tokenizer.model"\n    },\n    {\n      "filename": "special_tokens_map.json",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/special_tokens_map.json"\n    },\n    {\n      "filename": "tokenizer.json",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/tokenizer.json"\n    },\n    {\n      "filename": "tokenizer_config.json",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/tokenizer_config.json"\n    },\n    {\n      "filename": "model.cache",\n      "url": "https://delta.jan.ai/dist/models/<gpuarch>/<os>/tensorrt-llm-v0.7.1/TinyJensen-1.1B-Chat-fp16/model.cache"\n    }\n  ],\n  "id": "tinyjensen-1.1b-chat-fp16",\n  "object": "model",\n  "name": "TinyJensen 1.1B Chat FP16",\n  "version": "1.0",\n  "description": "Do you want to chat with Jensen Huan? Here you are",\n  "format": "TensorRT-LLM",\n  "settings": {\n    "ctx_len": 2048,\n    "text_model": false\n  },\n  "parameters": {\n    "max_tokens": 4096\n  },\n  "metadata": {\n    "author": "LLama",\n    "tags": [\n      "TensorRT-LLM",\n      "1B",\n      "Finetuned"\n    ],\n    "size": 2151000000\n  },\n  "engine": "nitro-tensorrt-llm"\n}\n'})})]}),"\n",(0,i.jsx)(n.h3,{id:"using-a-tensorrt-llm-model",children:"Using a TensorRT-LLM Model"}),"\n",(0,i.jsx)(n.p,{children:"You can just select and use a TensorRT-LLM model from Jan's Thread interface."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Jan will automatically start the TensorRT-LLM model engine in the background"}),"\n",(0,i.jsx)(n.li,{children:"You may encounter a pop-up from Windows Security, asking for Nitro to allow public and private network access"}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{title:"Why does Nitro need network access?",type:"info",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["This is because Jan runs TensorRT-LLM using the ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm/",children:"Nitro Server"})]}),"\n",(0,i.jsx)(n.li,{children:"Jan makes network calls to the Nitro server running on your computer on a separate port"}),"\n"]})}),"\n",(0,i.jsx)(n.h3,{id:"configure-settings",children:"Configure Settings"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"coming soon"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h2,{id:"extension-details",children:"Extension Details"}),"\n",(0,i.jsxs)(n.p,{children:["Jan's TensorRT-LLM Extension is built on top of the open source ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm",children:"Nitro TensorRT-LLM Server"}),", a C++ inference server on top of TensorRT-LLM that provides an OpenAI-compatible API."]}),"\n",(0,i.jsx)(n.h3,{id:"manual-build",children:"Manual Build"}),"\n",(0,i.jsxs)(n.p,{children:["To manually build the artifacts needed to run the server and TensorRT-LLM, you can reference the source code. ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm?tab=readme-ov-file#quickstart",children:"Read here"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"uninstall-extension",children:"Uninstall Extension"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Quit the app"}),"\n",(0,i.jsx)(n.li,{children:"Go to Settings > Extensions"}),"\n",(0,i.jsx)(n.li,{children:"Delete the entire Extensions folder."}),"\n",(0,i.jsx)(n.li,{children:"Reopen the app, only the default extensions should be restored."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"build-your-own-tensorrt-models",children:"Build your own TensorRT models"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"coming soon"})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(96540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);