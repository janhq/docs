"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1089],{57405:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});var l=t(85893),a=t(11151);const o={title:"Ollama",slug:"/guides/engines/ollama",sidebar_position:4,description:"A step-by-step guide on how to integrate Jan with Ollama.",keywords:["Jan","Rethink the Computer","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","Ollama integration"]},i=void 0,s={id:"guides/local-providers/ollama",title:"Ollama",description:"A step-by-step guide on how to integrate Jan with Ollama.",source:"@site/docs/guides/local-providers/ollama.mdx",sourceDirName:"guides/local-providers",slug:"/guides/engines/ollama",permalink:"/guides/engines/ollama",draft:!1,unlisted:!1,editUrl:"https://github.com/janhq/jan/tree/dev/docs/docs/guides/local-providers/ollama.mdx",tags:[],version:"current",lastUpdatedBy:"hiento09",lastUpdatedAt:1711094318,formattedLastUpdatedAt:"Mar 22, 2024",sidebarPosition:4,frontMatter:{title:"Ollama",slug:"/guides/engines/ollama",sidebar_position:4,description:"A step-by-step guide on how to integrate Jan with Ollama.",keywords:["Jan","Rethink the Computer","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","Ollama integration"]},sidebar:"guidesSidebar",previous:{title:"LM Studio",permalink:"/guides/engines/lmstudio"},next:{title:"TensorRT-LLM Extension",permalink:"/guides/engines/tensorrt-llm"}},r={},d=[{value:"Integrate Ollama with Jan",id:"integrate-ollama-with-jan",level:2},{value:"Step 1: Start the Ollama Server",id:"step-1-start-the-ollama-server",level:3},{value:"Step 2: Model Configuration",id:"step-2-model-configuration",level:3},{value:"Step 3: Start the Model",id:"step-3-start-the-model",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.h2,{id:"integrate-ollama-with-jan",children:"Integrate Ollama with Jan"}),"\n",(0,l.jsx)(n.p,{children:"Ollama provides you with largen language that you can run locally. There are two methods to integrate Ollama with Jan:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Integrate Ollama server with Jan."}),"\n",(0,l.jsx)(n.li,{children:"Migrate the downloaded model from Ollama to Jan."}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"To integrate Ollama with Jan, follow the steps below:"}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["In this tutorial, we'll show how to integrate Ollama with Jan using the first method. We will use the ",(0,l.jsx)(n.a,{href:"https://ollama.com/library/llama2",children:"llama2"})," model as an example."]})}),"\n",(0,l.jsx)(n.h3,{id:"step-1-start-the-ollama-server",children:"Step 1: Start the Ollama Server"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Choose your model from the ",(0,l.jsx)(n.a,{href:"https://ollama.com/library",children:"Ollama library"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"Run your model with this command:"}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-sh",children:"ollama run <model-name>\n"})}),"\n",(0,l.jsxs)(n.ol,{start:"3",children:["\n",(0,l.jsxs)(n.li,{children:["According to the ",(0,l.jsx)(n.a,{href:"https://github.com/ollama/ollama/blob/main/docs/openai.md",children:"Ollama documentation on OpenAI compatibility"}),", you can connect to the Ollama server using the web address ",(0,l.jsx)(n.code,{children:"http://localhost:11434/v1/chat/completions"}),". To do this, change the ",(0,l.jsx)(n.code,{children:"openai.json"})," file in the ",(0,l.jsx)(n.code,{children:"~/jan/engines"})," folder to add the Ollama server's full web address:"]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",metastring:'title="~/jan/engines/openai.json"',children:'{\n  "full_url": "http://localhost:11434/v1/chat/completions"\n}\n'})}),"\n",(0,l.jsx)(n.h3,{id:"step-2-model-configuration",children:"Step 2: Model Configuration"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Navigate to the ",(0,l.jsx)(n.code,{children:"~/jan/models"})," folder."]}),"\n",(0,l.jsxs)(n.li,{children:["Create a folder named ",(0,l.jsx)(n.code,{children:"(ollam-modelname)"}),", for example, ",(0,l.jsx)(n.code,{children:"lmstudio-phi-2"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Create a ",(0,l.jsx)(n.code,{children:"model.json"})," file inside the folder including the following configurations:"]}),"\n"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Set the ",(0,l.jsx)(n.code,{children:"id"})," property to the model name as Ollama model name."]}),"\n",(0,l.jsxs)(n.li,{children:["Set the ",(0,l.jsx)(n.code,{children:"format"})," property to ",(0,l.jsx)(n.code,{children:"api"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Set the ",(0,l.jsx)(n.code,{children:"engine"})," property to ",(0,l.jsx)(n.code,{children:"openai"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Set the ",(0,l.jsx)(n.code,{children:"state"})," property to ",(0,l.jsx)(n.code,{children:"ready"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-json",metastring:'title="~/jan/models/llama2/model.json"',children:'{\n  "sources": [\n    {\n      "filename": "llama2",\n      "url": "https://ollama.com/library/llama2"\n    }\n  ],\n  "id": "llama2",\n  "object": "model",\n  "name": "Ollama - Llama2",\n  "version": "1.0",\n  "description": "Llama 2 is a collection of foundation language models ranging from 7B to 70B parameters.",\n  "format": "api",\n  "settings": {},\n  "parameters": {},\n  "metadata": {\n    "author": "Meta",\n    "tags": ["General", "Big Context Length"]\n  },\n  "engine": "openai"\n}\n'})}),"\n",(0,l.jsx)(n.admonition,{type:"note",children:(0,l.jsxs)(n.p,{children:["For more details regarding the ",(0,l.jsx)(n.code,{children:"model.json"})," settings and parameters fields, please see ",(0,l.jsx)(n.a,{href:"/guides/engines/remote-server/#modeljson",children:"here"}),"."]})}),"\n",(0,l.jsx)(n.h3,{id:"step-3-start-the-model",children:"Step 3: Start the Model"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Restart Jan and navigate to the ",(0,l.jsx)(n.strong,{children:"Hub"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Locate your model and click the ",(0,l.jsx)(n.strong,{children:"Use"})," button."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.a)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>s,a:()=>i});var l=t(67294);const a={},o=l.createContext(a);function i(e){const n=l.useContext(o);return l.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),l.createElement(o.Provider,{value:n},e.children)}}}]);