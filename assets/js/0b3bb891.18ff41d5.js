"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9754],{87062:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var i=s(74848),t=s(28453);const o={title:"TensorRT-LLM Extension",slug:"/guides/engines/tensorrt-llm",sidebar_position:2,description:"A step-by-step guide on how to customize the TensorRT-LLM extension.",keywords:["Jan","Customizable Intelligence","LLM","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","TensorRT-LLM Extension","TensorRT","tensorRT","extension"]},r=void 0,l={id:"guides/local-providers/tensorrt",title:"TensorRT-LLM Extension",description:"A step-by-step guide on how to customize the TensorRT-LLM extension.",source:"@site/docs/guides/local-providers/tensorrt.mdx",sourceDirName:"guides/local-providers",slug:"/guides/engines/tensorrt-llm",permalink:"/guides/engines/tensorrt-llm",draft:!1,unlisted:!1,editUrl:"https://github.com/janhq/docs/tree/main/docs/guides/local-providers/tensorrt.mdx",tags:[],version:"current",lastUpdatedBy:"Nicole Zhu",lastUpdatedAt:1712046709,formattedLastUpdatedAt:"Apr 2, 2024",sidebarPosition:2,frontMatter:{title:"TensorRT-LLM Extension",slug:"/guides/engines/tensorrt-llm",sidebar_position:2,description:"A step-by-step guide on how to customize the TensorRT-LLM extension.",keywords:["Jan","Customizable Intelligence","LLM","local AI","privacy focus","free and open source","private and offline","conversational AI","no-subscription fee","large language models","TensorRT-LLM Extension","TensorRT","tensorRT","extension"]},sidebar:"guidesSidebar",previous:{title:"Llama.cpp Extension",permalink:"/guides/engines/llamacpp"},next:{title:"LM Studio",permalink:"/guides/engines/lmstudio"}},a={},d=[{value:"Overview",id:"overview",level:2},{value:"TensortRT-LLM Extension",id:"tensortrt-llm-extension",level:2},{value:"Pre-requisites",id:"pre-requisites",level:3},{value:"Step 1: Install TensorRT-Extension",id:"step-1-install-tensorrt-extension",level:3},{value:"Step 2: Download a Compatible Model",id:"step-2-download-a-compatible-model",level:3},{value:"Step 3: Configure Settings",id:"step-3-configure-settings",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Incompatible Extension vs Engine versions",id:"incompatible-extension-vs-engine-versions",level:3},{value:"Uninstall Extension",id:"uninstall-extension",level:3},{value:"Install Nitro-TensorRT-LLM manually",id:"install-nitro-tensorrt-llm-manually",level:3},{value:"Build your own TensorRT models",id:"build-your-own-tensorrt-models",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["Users with Nvidia GPUs can get ",(0,i.jsx)(n.strong,{children:"20-40% faster token speeds"})," compared to using LlamaCPP engine on their laptop or desktops by using ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"}),". The greater implication is that you are running FP16, which is also more accurate than quantized models."]}),"\n",(0,i.jsx)(n.h2,{id:"tensortrt-llm-extension",children:"TensortRT-LLM Extension"}),"\n",(0,i.jsxs)(n.p,{children:["This guide walks you through how to install Jan's official ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm",children:"TensorRT-LLM Extension"}),". This extension uses ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm",children:"Nitro-TensorRT-LLM"})," as the AI engine, instead of the default ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro",children:"Nitro-Llama-CPP"}),". It includes an efficient C++ server to natively execute the ",(0,i.jsx)(n.a,{href:"https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html",children:"TRT-LLM C++ runtime"}),". It also comes with additional feature and performance improvements like OpenAI compatibility, tokenizer improvements, and queues."]}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"This feature is only available for Windows users. Linux is coming soon."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Additionally, we only prebuilt a few demo models. You can always build your desired models directly on your machine. For more information, please see ",(0,i.jsx)(n.a,{href:"#build-your-own-tensorrt-models",children:"here"}),"."]}),"\n"]}),"\n"]})}),"\n",(0,i.jsx)(n.h3,{id:"pre-requisites",children:"Pre-requisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"A Windows PC"}),"\n",(0,i.jsx)(n.li,{children:"Nvidia GPU(s): Ada or Ampere series (i.e. RTX 4000s & 3000s). More will be supported soon."}),"\n",(0,i.jsx)(n.li,{children:"3GB+ of disk space to download TRT-LLM artifacts and a Nitro binary"}),"\n",(0,i.jsx)(n.li,{children:"Jan v0.4.9+ or Jan v0.4.8-321+ (nightly)"}),"\n",(0,i.jsxs)(n.li,{children:["Nvidia Driver v535+ (For installation guide, please see ",(0,i.jsx)(n.a,{href:"/troubleshooting/#1-ensure-gpu-mode-requirements",children:"here"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["CUDA Toolkit v12.2+ (For installation guide, please see ",(0,i.jsx)(n.a,{href:"/troubleshooting/#1-ensure-gpu-mode-requirements",children:"here"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-1-install-tensorrt-extension",children:"Step 1: Install TensorRT-Extension"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Go to ",(0,i.jsx)(n.strong,{children:"Settings"})," > ",(0,i.jsx)(n.strong,{children:"Extensions"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Click ",(0,i.jsx)(n.strong,{children:"Install"})," next to the TensorRT-LLM Extension."]}),"\n",(0,i.jsx)(n.li,{children:"Check that files are correctly downloaded."}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-sh",children:"ls ~\\jan\\extensions\\@janhq\\tensorrt-llm-extension\\dist\\bin\n# Your Extension Folder should now include `nitro.exe`, among other artifacts needed to run TRT-LLM\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-download-a-compatible-model",children:"Step 2: Download a Compatible Model"}),"\n",(0,i.jsxs)(n.p,{children:["TensorRT-LLM can only run models in ",(0,i.jsx)(n.code,{children:"TensorRT"}),' format. These models, aka "TensorRT Engines", are prebuilt specifically for each target OS+GPU architecture.']}),"\n",(0,i.jsx)(n.p,{children:"We offer a handful of precompiled models for Ampere and Ada cards that you can immediately download and play with:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Restart the application and go to the Hub."}),"\n",(0,i.jsxs)(n.li,{children:["Look for models with the ",(0,i.jsx)(n.code,{children:"TensorRT-LLM"})," label in the recommended models list > Click ",(0,i.jsx)(n.strong,{children:"Download"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"This step might take some time. \ud83d\ude4f"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://hackmd.io/_uploads/rJewrEgRp.png",alt:"image"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsx)(n.li,{children:"Click use and start chatting!"}),"\n",(0,i.jsx)(n.li,{children:"You may need to allow Nitro in your network"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"alt text",src:s(61914).A+"",width:"679",height:"428"})}),"\n",(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsx)(n.p,{children:"If you are our nightly builds, you may have to reinstall the TensorRT-LLM extension each time you update the app. We're working on better extension lifecyles - stay tuned."})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-configure-settings",children:"Step 3: Configure Settings"}),"\n",(0,i.jsx)(n.p,{children:"You can customize the default parameters for how Jan runs TensorRT-LLM."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"coming soon"})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"incompatible-extension-vs-engine-versions",children:"Incompatible Extension vs Engine versions"}),"\n",(0,i.jsx)(n.p,{children:"For now, the model versions are pinned to the extension versions."}),"\n",(0,i.jsx)(n.h3,{id:"uninstall-extension",children:"Uninstall Extension"}),"\n",(0,i.jsx)(n.p,{children:"To uninstall the extension, follow the steps below:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Quit the app."}),"\n",(0,i.jsxs)(n.li,{children:["Go to ",(0,i.jsx)(n.strong,{children:"Settings"})," > ",(0,i.jsx)(n.strong,{children:"Extensions"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Delete the entire Extensions folder."}),"\n",(0,i.jsx)(n.li,{children:"Reopen the app, only the default extensions should be restored."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"install-nitro-tensorrt-llm-manually",children:"Install Nitro-TensorRT-LLM manually"}),"\n",(0,i.jsxs)(n.p,{children:["To manually build the artifacts needed to run the server and TensorRT-LLM, you can reference the source code. ",(0,i.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm?tab=readme-ov-file#quickstart",children:"Read here"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"build-your-own-tensorrt-models",children:"Build your own TensorRT models"}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"coming soon"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},61914:(e,n,s)=>{s.d(n,{A:()=>i});const i=s.p+"assets/images/image-43682306e0e1318012556ba5111c1b38.png"},28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var i=s(96540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);