---
title: LLM hardware for infererence
description: Hardware for infererence
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Discord integration,
    Discord,
    bot,
  ]
---

import { Tabs } from 'nextra/components'

## What is LLM Inference
LLM inference is the process of entering a prompt and generating a response from an LLM. It involves a language model drawing conclusions or making predictions to generate an appropriate output based on the patterns and relationships to which it was exposed during training. 

Firstly, in the prefill phase, the LLM must process the text from a user’s input prompt by converting it into a series of prompt, or input, tokens. A token is a unit of text that represents a word or a portion of a word. For the English language, a token is 0.75 words – or four characters.  The exact mechanism that an LLM uses to divide text into tokens, i.e., its tokenizer, varies between models. Once generated, each token is turned into a vector embedding, a numerical representation that the model can understand and make inferences from. These embeddings are then processed by the LLM in order to generate an appropriate output for the user. 

From here, during the decoding phase, the LLM generates a series of vector embeddings that represent its response to the given input prompt. These are then converted into completion, or output, tokens, which are generated output one at a time until it reaches a stopping criterion, such as the token limit number or one of a list of stop words. At which time, it will generate a special end token to signal the end of token generation. As LLMs generate one token per forward propagation, i.e., pass or iteration, the number of propagations that a model requires to complete a response is the same as the number of completion tokens.

## What Are the Most Important LLM Inference Performance Metrics?
To evaluate the inference capabilities of a large language model, the metrics that we’re most interested in are latency and throughput. 

### Latency
Latency is a measure of how long it takes for an LLM to generate a response to a user’s prompt. It provides a way to evaluate a language model’s speed and is mainly responsible for forming a user’s impression of how fast or efficient a generative AI application is. Consequently, low latency is important for use cases that involve real-time interactions, such as chatbots and AI copilots, but less so for offline processes. There are several ways to measure a model’s latency, including: 

- Time To First Token (TTFT):
- Time Per Output Token (TPOT)
- Total generation time
**TTFT** is the length of time it takes for the user to start receiving a response from a model after entering their prompt. It’s determined by the time it takes to process the user’s input and generate the first completion token. Factors that influence TFTT include:

- Network speed: a system’s general bandwidth and, similarly, how congested the network is at the time of inference.  
- Input sequence length: the longer the prompt, the more processing required by the model before it can output the first token.   
- Model size: conventionally, the larger the model, i.e., the more parameters it has, the more computations it performs to generate a response, which prolongs the TFTT. 
**TPOT**, alternatively, is the average time it takes to generate a completion token for each user querying the model at a given time. This can also occasionally be referred to as inter-token latency (ITL). 

Total generation time refers to the end-to-end latency of an LLM: from when a prompt is originally entered by the user to when they receive the completed output from the model; often, when people refer to latency, they’re actually referring to total generation time. It can be calculated as follows:

**Total generation time =  TTFT + (TPOT x number of generated tokens)**
An LLM’s total generation time varies according to a number of key factors:

- Output Length: this is the most important factor because models generate output a token at a time. This is also why the LLM’s TPOT should also be measured. 
- Prefill Time:  the time it takes for the model to complete the prefill stage, i.e., how long it takes the model to process all the input tokens from the user’s entered prompt and can generate the first completion token. 
- Queuing Time: there may be times when an LLM can’t keep up with user requests because of its hardware constraints – namely a lack of GPU memory. This means some input requests will be placed in a queue before they’re processed. This is the reason behind TTFT being such a commonly recorded metric, as it offers insight into how well the model’s server can handle varying numbers of user requests and, subsequently, how it might perform in a real-world setting.
Something else to be considered when measuring latency is the concept of a cold start. When an LLM is invoked after previously being inactive, i.e., scaled to zero, it causes a “cold” start as the model’s server must create an instance to process the request. This has a considerable effect on latency measurements – particularly TFTT and total generation time, so it’s crucial to note whether the published inference monitoring results for a model specify whether they include a cold start time or not.

**Throughput**
An LLM’s throughput provides a measure of how many requests it can process or how much output it can produce in a given time span. Throughput is typically measured in two ways: requests per second or tokens per second.

- Requests per second: this metric is dependent on the model’s total generation time and how many requests are being made at the same time, i.e., how well the model handles concurrency. However, total generation time varies based on how long the model’s input and output are.
- Tokens per second: because requests per second are influenced by total generation time, which itself depends on the length of the model’s output and, to a lesser extent, its input, tokens per second is a more commonly used metric for measuring throughput. Much like TFTT, the tokens per second metric is integral to the perceived speed of an LLM.
Additionally, tokens per second could refer to:

- Total tokens per second: both input and output tokens
- Output tokens per second: only generated completion tokens
Typically, total tokens per second is considered the more definitive measure of model throughput, while output tokens per second is applicable to measuring the performance of LLMs for use in real-time applications. 

**Request Batching**
One of the most effective and increasingly employed methods for increasing an LLM’s throughput is batching. Instead of loading the model’s parameters for each user prompt, batching involves collecting as many inputs as possible to process at once – so parameters have to be loaded less frequently. However, while this makes the most efficient use of a GPU and improves throughput, it does so at the expense of latency – as users that made the initial requests that comprise a batch will have to wait until it’s processed to receive a response. What’s more, the larger the batch size, the bigger the drop-off in latency, although there are limits on the maximum size of a batch before causing memory overflow. 

Types of batching techniques include:  

- Static batching: also called naïve batching, this is the default batching method with which multiple prompts are gathered and responses are only generated when all the requests in the batch are complete.
- Continuous batching: also known as in-flight batching; as opposed to waiting for all the prompts within a batch to be completed, this form of batching groups requests at the iteration level. As a result, once a request has been completed, a new one can replace it, making it more compute-efficient.  

## How is the hosted LLM performance
The site GTP For Work features a latency tracker that continually monitors the performance of the APIs for several models from OpenAI and Azure OpenAI (GPT-4 and GPT-3.5 and Anthropic (Claude 1.0 and 2.0). They publish the average latency of each model over a 48-hour period, based on:

- Generating a maximum of 512 tokens
- A temperature of 0.7 
- 10 minute intervals
- Data from three locations

| Model            | Throughput (tokens per second) | Latency (TRT) (seconds) | Latency (TFCR) (seconds) |
|------------------|--------------------------------|--------------------------|---------------------------|
| Mixtral 8x7B     | 95                             | 2.66                     | 0.6                       |
| GPT-3.5 Turbo    | 92                             | 1.85                     | 0.65                      |
| Gemini Pro       | 86                             | 3.6                      | 2.6                       |
| Llama 2 Chat (70B) | 82                           | 3.16                     | 0.88                      |
| Claude 2.0       | 27                             | 4.8                      | 0.9                       |
| GPT-4            | 22                             | 7.35                     | 1.9                       |
| GPT-4 Turbo      | 20                             | 7.05                     | 1.05                      |
| Mistral Medium   | 19                             | 6.2                      | 0.3                       |

## How is local LLM performance
<Tabs items={['1 x L4 GPU', '2 x L4 GPUs', '4 x L4 GPUs', '8 x L4 GPUs']}>
  <Tabs.Tab>
  | Model                 | Throughput (tokens per second) | Throughput (requests per second) | Average latency (seconds) | Average latency per token (seconds) | Average latency per output token (seconds) | Total time (seconds) |
  |-----------------------|--------------------------------|----------------------------------|----------------------------|--------------------------------------|--------------------------------------------|----------------------|
  | Llama2-7B             | 558.54                         | 1.17                             | 449.88                     | 1.71                                 | 10.87                                      | 897.23               |
  | Mistral-7B-instruct   | 915.48                         | 1.89                             | 277.19                     | 0.97                                 | 7.12                                       | 552.44               |
  </Tabs.Tab>
  <Tabs.Tab>
  | Model                 | Throughput (tokens per second) | Throughput (requests per second) | Average latency (seconds) | Average latency per token (seconds) | Average latency per output token (seconds) | Total time (seconds) |
  |-----------------------|--------------------------------|----------------------------------|----------------------------|--------------------------------------|--------------------------------------------|----------------------|
  | Llama2-7B             | 1265.17 |	2.65 |	179.85	| 0.63 |	3.81 |	397.65               |
  | Mistral-7B-instruct   | 1625.08	| 3.35 |	153.09 |	0.50 |	2.65 |	339.51               |
  </Tabs.Tab>
  <Tabs.Tab>
  | Model                 | Throughput (tokens per second) | Throughput (requests per second) | Average latency (seconds) | Average latency per token (seconds) | Average latency per output token (seconds) | Total time (seconds) |
  |-----------------------|--------------------------------|----------------------------------|----------------------------|--------------------------------------|--------------------------------------------|----------------------|
  | Llama2-7B             | 1489.99 |	3.12 |	147.36 |	0.48 |	2.57 |	324.71               |
  | Mistral-7B-instruct   | 1742.70 |	3.59 |	136.49 |	0.44 |	2.68 |	285.03               |
  </Tabs.Tab>
  <Tabs.Tab>
  | Model                 | Throughput (tokens per second) | Throughput (requests per second) | Average latency (seconds) | Average latency per token (seconds) | Average latency per output token (seconds) | Total time (seconds) |
  |-----------------------|--------------------------------|----------------------------------|----------------------------|--------------------------------------|--------------------------------------------|----------------------|
  | Llama2-7B             | 1401.18 |	2.93 |	153.09 |	0.50 |	2.65 |	339.51               |
  | Mistral-7B-instruct   | 1570.70 |	3.24 |	149.67 |	0.48 |	2.90 |	316.74               |
  | Llama2-70B |	– |	1.00 |	475.59 |	1.62 |	9.21 |	996.86 |
  </Tabs.Tab>
</Tabs>

## What determines hardware for LLM Inference:
- Possible bottlenecks
  - Motherboard extension for GPU - CPU: PCIe version and lane. Choose PCIe 4.0 x16 lanes or 5.0
  - DIMM memory: DDR4 or DDR5
  - GPU: Number of CUDA/ TensorCore with VRAM bandwidth
  - PSU
  - Thermal condition: Hot temperature -> GPU/ CPU throttle
