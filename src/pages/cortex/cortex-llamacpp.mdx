---
title: Cortex.llamacpp
description: Cortex.llamacpp Architecture
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Cortex,
    Jan,
    LLMs
  ]
---

import { Callout, Steps } from 'nextra/components'
import { Cards, Card } from 'nextra/components'

<Callout type="warning">
ðŸš§ Cortex is under construction.
</Callout>
# Cortex.llamacpp

Cortex.llamacpp is a C++ inference library that can be loaded by any server at runtime. It submodules (and occasionally upstreams) [llama.cpp](https://github.com/ggerganov/llama.cpp) for GGUF inference. 

In addition to llama.cpp, Cortex.llamacpp adds: 
- OpenAI compatibility for the stateless endpoints
- Model warm up
- Model orchestration optimizations

<Callout type="info">
Cortex.llamacpp is formerly called "Nitro".
</Callout>

If you already use [Jan](/docs) or [Cortex](/cortex), cortex.llamacpp is bundled by default and you donâ€™t need this guide. This guides walks you through how to use cortex.llamacpp as a standalone library, in any custom C++ server.

## Usage

To include cortex.llamacpp in your own server implementation, follow this [server example](https://github.com/janhq/cortex.llamacpp/tree/main/examples/server).

## Interface

Cortex.llamacpp has the following Interfaces:

- **HandleChatCompletion:** Processes chat completion tasks
    
    ```cpp
    void HandleChatCompletion(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **HandleEmbedding:** Generates embeddings for the input data provided
    
    ```cpp
    void HandleEmbedding(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **LoadModel:** Loads a model based on the specifications
    
    ```cpp
    void LoadModel(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **UnloadModel:** Unloads a model as specified
    
    ```cpp
    void UnloadModel(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
- **GetModelStatus:** Retrieves the status of a model
    
    ```cpp
    void GetModelStatus(
          std::shared_ptr<Json::Value> jsonBody,
          std::function<void(Json::Value&&, Json::Value&&)>&& callback);
    ```
    
**Parameters:**

- **`jsonBody`**: The request content in JSON format.
- **`callback`**: A function that handles the response

## Architecture

```mermaid
```

Interaction between components to provide an API for `embedding` and `inference` tasks using the **`llama.cpp`** library.

Main components:

- `cortex.cpp`: responsible for handling API requests and responses.
- `cortex.llamacpp`: makes APIs accessible through an Engine Interface, allowing others to easily use its features.
- **`enginei` i**mplements detailed logic for all **`llama engine`** APIs, handling endpoint logic and facilitating communication between **`cortex.cpp`** and **`llama engine`**.
- **`llama engine` :** exposes APIs for embedding and inference. It loads and unloads models and simplifies API calls to **`llama.cpp`**.
- **`llama.cpp` : a** submodule from the **`llama.cpp`** repository that provides the core functionality for embeddings and inferences.
- **`llama server context` :** a wrapper offers a simpler and more user-friendly interface for **`llama.cpp`** APIs

Communication:

```mermaid
```

- Streaming: When streaming is enabled, the response is processed and returned as chunks of results. This is done by creating tokens through inferencing, one token at a time.
- Non-streaming: the response is processed as a whole. After **`llama server context`** completes the entire process, it returns a single result back to **`cortex.cpp`**.

## Code Structure

The source code is available at [@janhq/cortex.llamacpp](https://github.com/janhq/cortex.llamacpp)

```markdown
.
â”œâ”€â”€ base                              **# Engine interface definition**
|   â””â”€â”€ cortex-common                 # Common interfaces used for all engines
|      â””â”€â”€ enginei.h                  # Define abstract classes and interface methods for engines
â”œâ”€â”€ examples                          **# Server example to integrate engine**
â”‚   â””â”€â”€ server.cc                     # Example server demonstrating engine integration
â”œâ”€â”€ llama.cpp                         **# Upstream llama.cpp repository**
â”‚   â””â”€â”€ (files from upstream llama.cpp)
â”œâ”€â”€ src                               **# Source implementation for llama.cpp**
â”‚   â”œâ”€â”€ chat_completion_request.h     # OpenAI compatible request handling
â”‚   â”œâ”€â”€ llama_client_slot             # Manage vector of slots for parallel processing
â”‚   â”œâ”€â”€ llama_engine                  # Implementation llamacpp engine of model loading and inference 
â”‚   â”œâ”€â”€ llama_server_context          # Context management for chat completion requests
â”‚   â”‚   â”œâ”€â”€ slot                      # Struct for slot management
â”‚   â”‚   â””â”€â”€ llama_context             # Struct for llama context management
|   |   â””â”€â”€ chat_completion           # Struct for chat completion management
|   |   â””â”€â”€ embedding                 # Struct for embedding management
â”œâ”€â”€ third-party                       **# Dependencies of the cortex.llamacpp project**
â”‚   â””â”€â”€ (list of third-party dependencies)

```

## Runtime

## Roadmap
