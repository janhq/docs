---
title: llama.cpp
description: A step-by-step guide on how to customize the llama.cpp engine.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Engine,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---

import { Tabs } from 'nextra/components'
import { Callout, Steps } from 'nextra/components'

# llama.cpp (Default)

## Overview

Jan has a default [C++ inference server](https://github.com/janhq/cortex) built on top of [llama.cpp](https://github.com/ggerganov/llama.cpp). This server provides an OpenAI-compatible API, queues, scaling, and additional features on top of the wide capabilities of `llama.cpp`.

## llama.cpp Engine

This guide shows you how to initialize the `llama.cpp` to download and install the required dependencies to start chatting with a model using the `llama.cpp` engine.

## Prerequisites
- Mac Intel:
  - Make sure you're using an Intel-based Mac. For a complete list of supported Intel CPUs, please see [here](https://en.wikipedia.org/wiki/MacBook_Pro_(Intel-based)).
  - For Mac Intel, it is recommended to utilize smaller models.
- Mac Sillicon:
  - Make sure you're using a Mac Silicon. For a complete list of supported Apple Silicon CPUs, please see [here](https://en.wikipedia.org/wiki/Apple_Silicon).
  - Using an adequate model size based on your hardware is recommended for Mac Silicon.
<Callout type="info">
  This can use Apple GPU with Metal by default for acceleration. Apple ANE is not supported yet.
</Callout>
- Windows:
  - Ensure that you have **Windows with x86_64** architecture.
- Linux:
  - Ensure that you have **Linux with x86_64** architecture.

## Step-by-step Guide
<Steps>
### Step 1: Initialize the Engine
1. Navigate to the **Advanced Settings** > enable the **Experimental Mode**.
2. Click the **Engines** tab.
3. Select **cortex.llamacpp** engine > click the **Install** button.
4. This downloads the required dependencies and data to run a model using `llama.cpp` engine.

### Step 2: Start the Model
1. Restart the Jan application to make sure the engine is correctly installed.
2. Navigate to Hub download a `GGUF` format model.
3. Go to the **Threads** > **On-device** > select the downloaded model.
4. Configure your model's parameters.
5. Chat with your model.
</Steps>

<Callout type='info'>
  If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.
</Callout>
