---
title: TensorRT-LLM
description: A step-by-step guide on customizing the TensorRT-LLM engine.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    TensorRT-LLM Engine,
    TensorRT,
    tensorRT,
    engine,
  ]
---

import { Callout, Steps } from 'nextra/components'

# TensorRT-LLM

## Overview

This guide walks you through installing Jan's official [TensorRT-LLM Engine](https://github.com/janhq/nitro-tensorrt-llm). This engine uses [Cortex-TensorRT-LLM](https://github.com/janhq/cortex.tensorrt-llm) as the AI engine instead of the default [Cortex-Llama-CPP](https://github.com/janhq/cortex).  It includes an efficient C++ server that executes the [TRT-LLM C++ runtime](https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html) natively. It also includes features and performance improvements like OpenAI compatibility, tokenizer improvements, and queues.

<Callout type='warning' emoji="">
 This feature is only available for Windows users. Linux is coming soon.
</Callout>

### Pre-requisites

- A **Windows** PC.
- **Nvidia GPU(s)**: Ada or Ampere series (i.e. RTX 4000s & 3000s). More will be supported soon.
- Sufficient disk space for the TensorRT-LLM models and data files (space requirements vary depending on the model size).


## Step-by-step Guide
<Steps>
### Step 1: Initialize the Engine
1. Navigate to the **Advanced Settings** > enable the **Experimental Mode**.
2. Click the **Engines** tab.
3. Select **cortex.tensorrt-llm** engine > click the **Install** button.
4. This downloads the required dependencies and data to run a model using `TensorRT-LLM` engine.

### Step 2: Start the Model
1. Restart the Jan application to make sure the engine is correctly installed.
2. Navigate to Hub download a `TensorRT-LLM` format model.
3. Go to the **Threads** > **On-device** > select the downloaded model.
4. Configure your model's parameters.
<Callout type='info'>
Ensure these parameters are aligned correctly to avoid runtime issues and fully leverage TensorRT engines' capabilities.
</Callout>
5. Chat with your model.
</Steps>

<Callout type='info'>
  If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.
</Callout>