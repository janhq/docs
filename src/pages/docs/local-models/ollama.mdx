---
title: Ollama
description: A step-by-step guide on how to integrate Jan with Ollama.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Ollama integration,
  ]
---

import { Callout, Steps } from 'nextra/components'

# Ollama

## Integrate Ollama with Jan

Ollama provides you with large language models that you can run locally. There are two methods to integrate Ollama with Jan:

1. Integrate the Ollama server with Jan.
2. Migrate the downloaded model from Ollama to Jan.

To integrate Ollama with Jan, follow the steps below:

<Callout type="info">
  This tutorial will show how to integrate Ollama with Jan using the first method. We will use the [llama2](https://ollama.com/library/llama2) model as an example.
</Callout>

<Steps>

### Step 1: Server Setup

According to the [Ollama documentation on OpenAI compatibility](https://github.com/ollama/ollama/blob/main/docs/openai.md), you can connect to the Ollama server using the web address `http://localhost:11434/v1/chat/completions`. To do this, follow the steps below:
1. Navigate to the **Settings** > **Extensions**.
2. In the **OpenAI Inference Engine** section, add the full web address of the Ollama server.
<br/>
![Server Setup](../_assets/server-llama2.gif)


<Callout type="info">
  Leave the API Key field blank.
</Callout>

### Step 2: Download Model

1. Navigate to the **Hub**.
2. Download the Ollama model, for example, `Llama 2 Chat 7B Q4`.
<br/>
![Download Model](../_assets/download-llama2.gif)

### Step 3: Start the Model

1. Navigate to the **Threads**.
2. Select the `Llama 2 Chat 7B Q4` model and configure the model parameters.
3. Start chatting with the model.
<br/>
![Start Model](../_assets/llama2.gif)

</Steps>