---
title: Server installation
description: Get started quickly with Jan, a ChatGPT-alternative that runs on your own computer, with a local API server. Learn how to install Jan and select an AI model to start chatting.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    quickstart,
    getting started,
    using AI model,
    installation,
    "server",
    "web"
  ]
---

import { Tabs, Callout, Steps } from 'nextra/components'

# Quickstart

To get started quickly with Jan, follow the steps below:

<Steps>
### Step 1: Prepare environment
<Tabs items={['On premise', 'AWS', 'GCP', 'Azure']}>
  <Tabs.Tab>
    - Choose a machine with a minimum of 16GB RAM, 8 CPU cores and 100GB storage
    - For better performance, you can use NVIDIA GPU (AMD GPU/ Intel Arc GPU are not supported yet)
  </Tabs.Tab>
  <Tabs.Tab>
    - Go to AWS console -> `EC2`
    - Choose instance with at least `c5.2xlarge` for CPU only or `g5.2xlarge` for NVIDIA GPU support
    - Add EBS volume with at least 100GB
    - Other options depends on your VPC/ security group settings (please allow inbound traffic on port 1377)
  </Tabs.Tab>
  <Tabs.Tab>
    - Go to GCP console -> `Compute instance`
    - Choose instance with at least `c2-standard-8` for CPU only or `g2-standard-4` for NVIDIA GPU support
    - Add EBS volume with at least 100GB
    - Other options depends on your VPC/ security group settings (please allow inbound traffic on port 1377)
  </Tabs.Tab>
  <Tabs.Tab>
    - Go to Azure console -> `Service` -> `Virtual machines`
    - Choose instance with at least `Standard_F8s_v2` for CPU only or `Standard_NC4as_T4_v3` for NVIDIA GPU support
    - Add EBS volume with at least 100GB
    - Other options depends on your VPC/ security group settings (please allow inbound traffic on port 1377)
  </Tabs.Tab>
</Tabs>
### Step 2: Get Jan Web

<Tabs items={['Linux', 'Windows WSL2']}>
  <Tabs.Tab>
    **Pre-requisites**
    Before installing Jan, ensure :

    To enable GPU support, you will need:
      - NVIDIA GPU with CUDA Toolkit 11.7 or higher
      - NVIDIA driver 470.63.01 or higher

    Jan runs in Docker mode on Linux:      
      - Docker Engine and Docker Compose are required to run Jan in Docker mode. Follow the [instructions](https://docs.docker.com/engine/install/ubuntu/) below to get started with Docker Engine on Ubuntu.
      ```bash
      curl -fsSL https://get.docker.com -o get-docker.sh
      sudo sh ./get-docker.sh --dry-run
      ```
      - Download Jan `docker-compose.yml` file
      ```bash
      curl https://raw.githubusercontent.com/janhq/jan/dev/docker-compose.yml -o docker-compose.yml
      ```
  - If you intend to run Jan in GPU mode, you need to install `nvidia-driver` and `nvidia-docker2`. Follow the instruction [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) for installation.

| Docker compose Profile | Description                                  |
| ---------------------- | -------------------------------------------- |
| `cpu-fs`               | Run Jan in CPU mode with default file system |
| `cpu-s3fs`             | Run Jan in CPU mode with S3 file system      |
| `gpu-fs`               | Run Jan in GPU mode with default file system |
| `gpu-s3fs`             | Run Jan in GPU mode with S3 file system      |

| Environment Variable    | Description                                                                                             |
| ----------------------- | ------------------------------------------------------------------------------------------------------- |
| `S3_BUCKET_NAME`        | S3 bucket name - leave blank for default file system                                                    |
| `AWS_ACCESS_KEY_ID`     | AWS access key ID - leave blank for default file system                                                 |
| `AWS_SECRET_ACCESS_KEY` | AWS secret access key - leave blank for default file system                                             |
| `AWS_ENDPOINT`          | AWS endpoint URL - leave blank for default file system                                                  |
| `AWS_REGION`            | AWS region - leave blank for default file system                                                        |
| `API_BASE_URL`          | Jan Server URL, please modify it as your public ip address or domain name default http://localhost:1377 |

- **Option 1**: Run Jan in CPU mode

  ```bash
  # cpu mode with default file system
  docker compose --profile cpu-fs up -d

  # cpu mode with S3 file system
  docker compose --profile cpu-s3fs up -d
  ```

- **Option 2**: Run Jan in GPU mode

  - **Step 1**: Check CUDA compatibility with your NVIDIA driver by running `nvidia-smi` and check the CUDA version in the output

    ```bash
    nvidia-smi

    # Output
    +---------------------------------------------------------------------------------------+
    | NVIDIA-SMI 531.18                 Driver Version: 531.18       CUDA Version: 12.1     |
    |-----------------------------------------+----------------------+----------------------+
    | GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                                         |                      |               MIG M. |
    |=========================================+======================+======================|
    |   0  NVIDIA GeForce RTX 4070 Ti    WDDM | 00000000:01:00.0  On |                  N/A |
    |  0%   44C    P8               16W / 285W|   1481MiB / 12282MiB |      2%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+
    |   1  NVIDIA GeForce GTX 1660 Ti    WDDM | 00000000:02:00.0 Off |                  N/A |
    |  0%   49C    P8               14W / 120W|      0MiB /  6144MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+
    |   2  NVIDIA GeForce GTX 1660 Ti    WDDM | 00000000:05:00.0 Off |                  N/A |
    | 29%   38C    P8               11W / 120W|      0MiB /  6144MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+

    +---------------------------------------------------------------------------------------+
    | Processes:                                                                            |
    |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
    |        ID   ID                                                             Usage      |
    |=======================================================================================|
    ```

  - **Step 2**: Visit [NVIDIA NGC Catalog ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda/tags) and find the smallest minor version of image tag that matches your CUDA version (e.g., 12.1 -> 12.1.0)

  - **Step 3**: Update the `Dockerfile.gpu` line number 5 with the latest minor version of the image tag from step 2 (e.g. change `FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04 AS base` to `FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS base`)

  - **Step 4**: Run command to start Jan in GPU mode

    ```bash
    # GPU mode with default file system
    docker compose --profile gpu-fs up -d

    # GPU mode with S3 file system
    docker compose --profile gpu-s3fs up -d
    ```

This will start the web server and you can access Jan at `http://localhost:3000`.

> Note: RAG feature is not supported in Docker mode with s3fs yet.

    #### Experimental Mode

    To enable the experimental mode, go to **Settings** > **Advanced Settings** and toggle the **Experimental Mode**

  </Tabs.Tab>

  <Tabs.Tab>
    **Pre-requisites**

    Ensure that your system meets the following requirements:
      - Windows 10 or higher is required to run Jan.
      - WSL2 is required to run Jan in Windows, follow this [instruction](https://learn.microsoft.com/en-us/windows/wsl/install) to install.

    To enable GPU support, you will need:
      - NVIDIA GPU with CUDA Toolkit 11.7 or higher
      - NVIDIA driver 470.63.01 or higher

    Jan runs in Docker mode on Windows WSL2:
    - Docker Engine and Docker Compose are required to run Jan in Docker mode. Follow the [instructions](https://docs.docker.com/engine/install/ubuntu/) below to get started with Docker Engine on Ubuntu.
    ```bash
    curl -fsSL https://get.docker.com -o get-docker.sh
    sudo sh ./get-docker.sh --dry-run
    ```
    - Download Jan `docker-compose.yml` file
    ```bash
    curl https://raw.githubusercontent.com/janhq/jan/dev/docker-compose.yml -o docker-compose.yml
    ```
  - If you intend to run Jan in GPU mode, you need to install `nvidia-driver` and `nvidia-docker2`. Follow the instruction [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) for installation.

| Docker compose Profile | Description                                  |
| ---------------------- | -------------------------------------------- |
| `cpu-fs`               | Run Jan in CPU mode with default file system |
| `cpu-s3fs`             | Run Jan in CPU mode with S3 file system      |
| `gpu-fs`               | Run Jan in GPU mode with default file system |
| `gpu-s3fs`             | Run Jan in GPU mode with S3 file system      |

| Environment Variable    | Description                                                                                             |
| ----------------------- | ------------------------------------------------------------------------------------------------------- |
| `S3_BUCKET_NAME`        | S3 bucket name - leave blank for default file system                                                    |
| `AWS_ACCESS_KEY_ID`     | AWS access key ID - leave blank for default file system                                                 |
| `AWS_SECRET_ACCESS_KEY` | AWS secret access key - leave blank for default file system                                             |
| `AWS_ENDPOINT`          | AWS endpoint URL - leave blank for default file system                                                  |
| `AWS_REGION`            | AWS region - leave blank for default file system                                                        |
| `API_BASE_URL`          | Jan Server URL, please modify it as your public ip address or domain name default http://localhost:1377 |

- **Option 1**: Run Jan in CPU mode

  ```bash
  # cpu mode with default file system
  docker compose --profile cpu-fs up -d

  # cpu mode with S3 file system
  docker compose --profile cpu-s3fs up -d
  ```

- **Option 2**: Run Jan in GPU mode

  - **Step 1**: Check CUDA compatibility with your NVIDIA driver by running `nvidia-smi` and check the CUDA version in the output

    ```bash
    nvidia-smi

    # Output
    +---------------------------------------------------------------------------------------+
    | NVIDIA-SMI 531.18                 Driver Version: 531.18       CUDA Version: 12.1     |
    |-----------------------------------------+----------------------+----------------------+
    | GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                                         |                      |               MIG M. |
    |=========================================+======================+======================|
    |   0  NVIDIA GeForce RTX 4070 Ti    WDDM | 00000000:01:00.0  On |                  N/A |
    |  0%   44C    P8               16W / 285W|   1481MiB / 12282MiB |      2%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+
    |   1  NVIDIA GeForce GTX 1660 Ti    WDDM | 00000000:02:00.0 Off |                  N/A |
    |  0%   49C    P8               14W / 120W|      0MiB /  6144MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+
    |   2  NVIDIA GeForce GTX 1660 Ti    WDDM | 00000000:05:00.0 Off |                  N/A |
    | 29%   38C    P8               11W / 120W|      0MiB /  6144MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+

    +---------------------------------------------------------------------------------------+
    | Processes:                                                                            |
    |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
    |        ID   ID                                                             Usage      |
    |=======================================================================================|
    ```

  - **Step 2**: Visit [NVIDIA NGC Catalog ](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda/tags) and find the smallest minor version of image tag that matches your CUDA version (e.g., 12.1 -> 12.1.0)

  - **Step 3**: Update the `Dockerfile.gpu` line number 5 with the latest minor version of the image tag from step 2 (e.g. change `FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04 AS base` to `FROM nvidia/cuda:12.1.0-runtime-ubuntu22.04 AS base`)

  - **Step 4**: Run command to start Jan in GPU mode

    ```bash
    # GPU mode with default file system
    docker compose --profile gpu-fs up -d

    # GPU mode with S3 file system
    docker compose --profile gpu-s3fs up -d
    ```

This will start the web server and you can access Jan at `http://localhost:3000`.

> Note: RAG feature is not supported in Docker mode with s3fs yet.

    #### Experimental Mode

    To enable the experimental mode, go to **Settings** > **Advanced Settings** and toggle the **Experimental Mode**

  </Tabs.Tab>

  <Tabs.Tab>
    **Pre-requisites**

    Ensure that your system meets the following requirements:
      - glibc 2.27 or higher (check with `ldd --version`)
      - gcc 11, g++ 11, cpp 11, or higher, refer to this link for more information.

    To enable GPU support, you will need:
      - NVIDIA GPU with CUDA Toolkit 11.7 or higher
      - NVIDIA driver 470.63.01 or higher

    #### Stable Releases

    To download stable releases, go to [Jan](https://jan.ai/) > select **Download for Linux**.

    The download should be available as a `.AppImage` file or a `.deb` file.

    #### Nightly Releases

    We provide the Nightly Release so that you can test new features and see what might be coming in a future stable release. Please be aware that there might be bugs!

    You can download it from [Jan's Discord](https://discord.gg/FTk2MvZwJH) in the [`#nightly-builds`](https://discord.gg/q8szebnxZ7) channel.

    #### Experimental Model

    To enable the experimental mode, go to **Settings** > **Advanced Settings** and toggle the **Experimental Mode**


    <Tabs items={['Linux', 'Debian / Ubuntu', 'Others']}>
      <Tabs.Tab> To install Jan, you should use your package manager's install or `dpkg`.</Tabs.Tab>

      <Tabs.Tab>
        To install Jan, run the following command:

        ```bash
        # Install Jan using dpkg
        sudo dpkg -i jan-linux-amd64-{version}.deb
        ```

        ```bash
        # Install Jan using apt-get
        sudo apt-get install ./jan-linux-amd64-{version}.deb
        # where jan-linux-amd64-{version}.deb is path to the Jan package
        ```

      </Tabs.Tab>

      <Tabs.Tab>
        To install Jan, run the following commands:

        ```bash
        # Install Jan using AppImage
        chmod +x jan-linux-x86_64-{version}.AppImage
        ./jan-linux-x86_64-{version}.AppImage
        # where jan-linux-x86_64-{version}.AppImage is path to the Jan package
        ```

      </Tabs.Tab>
    </Tabs>

    <Callout type='warning'>
    If you are stuck in a broken build, go to the [Broken Build](/guides/common-error/broken-build) section of Common Errors.
    </Callout>

  </Tabs.Tab>
</Tabs>

### Step 3: Download a Model

Jan provides a variety of local AI models tailored to different needs, ready for download. These models are installed and run directly on the user's device.

1. Go to the **Hub**.
2. Select the models that you would like to install, to see a model details click the dropdown button.
3. Click the **Download** button.

![Download a Model](./_assets/download.gif)

<Callout type="info">
  Ensure you select the appropriate model size by balancing performance, cost,
  and resource considerations in line with your task's specific requirements and
  hardware specifications.
</Callout>

### Step 4: Connect to ChatGPT (Optional)

Jan also provides access to remote models hosted on external servers, requiring an API key for connectivity. For example, to use the ChatGPT model with Jan, you must input your API key by following these steps:

1. Go to the **Thread** tab.
2. Under the Model dropdown menu, select the ChatGPT model.
3. Fill in your ChatGPT API Key that you can get in your [OpenAI platform](https://platform.openai.com/account/api-keys).

![Connect to ChatGPT](./_assets/gpt.gif)

### Step 5: Chat with Models

After downloading and configuring your model, you can immediately use it in the **Thread** tab.

![Chat with a model](./_assets/model.gif)

</Steps>

## Best Practices

This section outlines best practices for developers, analysts, and AI enthusiasts to enhance their experience with Jan when adding AI locally to their computers. Implementing these practices will optimize the performance of AI models.

### Follow the Quickstart Guide

The quickstart guide above is designed to facilitate a quick setup process. It provides a clear instruction and simple steps to get you up and running with Jan quickly. Even, if you are inexperienced in AI.

### Select the Right Models

Jan offers a range of pre-configured AI models that are suited for different purposes. You should identify which on that aligns with your objectives. There are factors to be considered:

- Capabilities
- Accuracy
- Processing Speed

<Callout type="info">
  - Some of these factors also depend on your hardware, please see Hardware
  Requirement. - Choosing the right model is important to achieve the best
  performance.
</Callout>

### Setting up Jan

Ensure that you familiarize yourself with the Jan application. Jan offers advanced settings that you can adjust. These settings may influence how your AI behaves locally. Please see the [Advanced Settings](/guides/advanced) article for a complete list of Jan's configurations and instructions on how to configure them.

### Integrations

Jan can work with many different systems and tools. Whether you are incorporating Jan with any open-source LLM provider or other tools, it is important to understand the integration capabilities and limitations.

### Mastering the Prompt Engineering

Prompt engineering is an important aspect when dealing with AI models to generate the desired outputs. Mastering this skill can significantly enhance the performance and the responses of the AI. Below are some tips that you can do for prompt engineering:

- Ask the model to adopt a persona
- Be specific and details get a more specific answers
- Provide examples or preference text or context at the beginning
- Use a clear and concise language
- Use certain keywords and phrases

## Pre-configured Models

To see the full list of Jan's pre-configured models, please see our official GitHub [here](https://github.com/janhq/jan).
