---
title: llama.cpp
description: A step-by-step guide on how to customize the llama.cpp extension.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---

import { Tabs } from 'nextra/components'
import { Callout } from 'nextra/components'

# llama.cpp

## Overview

[Nitro](https://github.com/janhq/nitro) is an inference server on top of [llama.cpp](https://github.com/ggerganov/llama.cpp). It provides an OpenAI-compatible API, queue, & scaling.

## llama.cpp Extension

<Callout type='info'>
  Nitro is the default AI engine downloaded with Jan. There is no additional setup needed.
</Callout>

In this guide, we'll walk you through the process of customizing your engine settings by configuring the `nitro.json` file

### Prerequisites
<Tabs items={['Mac', 'Windows', 'Linux']}>
    <Tabs.Tab >
      <Tabs items={['Mac Intel', 'Mac Silicon']}>
        <Tabs.Tab >
          Make sure you're using an Intel-based Mac. For a full list of supported Intel CPUs, please see [here](https://en.wikipedia.org/wiki/MacBook_Pro_(Intel-based)).
          
          <Callout type="info">
            This uses CPU by default and there is no acceleration option available. 
          </Callout>
          
          For Mac Intel, it is recommended to utilize smaller models. Expect the following outcomes:
           - High time to first token (ms) 
           - Low throughput (tokens/sec)
        </Tabs.Tab>

        <Tabs.Tab >
          Make sure you're using a Mac Silicon. For a full list of supported Apple Silicon CPUs, please see [here](https://en.wikipedia.org/wiki/Apple_Silicon)
          
          <Callout type="info">
            This can use Apple GPU with Metal by default for acceleration. Apple ANE is not supported yet.
          </Callout>

          For Mac Silicon, it is recommended to use an adequate model size based on your hardware. Expect the following outcomes:
          - Low time to first token (ms)
          - High throughput (tokens/sec)
        </Tabs.Tab>
        
      </Tabs>
      ## Step-by-step Guide
      1. Navigate to the `App Settings` > `Advanced` > `Open App Directory` > `~/jan/engine` folder.
    </Tabs.Tab>
    <Tabs.Tab >
      Currently, only Windows with x86_64 architecture is supported by default. 
      
      <Callout type="info">
        If you do not have any GPU/ NPU, it will use CPU by default.
      </Callout>
      
      For Windows, select a model size suited to your hardware by looking for the `Recommended RAM` tag in the Hub. Expect the following outcomes:
        - High time to first token (ms) 
        - Low throughput (tokens/sec)

      ### CPU Instruction
      Prebuilt Nitro with llama.cpp inference engine needs to run on the correct CPU instruction.
      Available CPU instruction options are:
      <Tabs items={['AVX2', 'AVX', 'AVX-512']}>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2) for more information.
          - AVX2 is the default instruction set for Intel CPUs and AMD CPUs in Jan.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) for more information.
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX-512](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX-512) for more information.
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
      </Tabs>
      ### Acceleration Options
      Available acceleration options are:
      <Tabs items={['NVIDIA GPU', 'AMD Radeon GPU', 'Intel Arc GPU']}>
        <Tabs.Tab >
          #### Prerequisites
            - Nvidia Driver v535+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
            - CUDA Toolkit v12.2+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
          #### How to Use
            - This feature is supported by `llama.cpp` with CUBLAS build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the NVIDIA GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          #### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          #### How to Use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the AMD Radeon GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          #### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          #### How to Use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the Intel Arc GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
      </Tabs>
      ## Step-by-step Guide
      1. Navigate to the `App Settings` > `Advanced` > `Open App Directory` > `~/<YOUR_USERNAME>/jan/engine` folder.
    </Tabs.Tab>
    <Tabs.Tab >
        Currently, only Linux with x86_64 architecture is supported.

      <Callout type="info">
        If you do not have any GPU/ NPU, it will use CPU by default.
      </Callout>

     For Linux, select a model size suited to your hardware based on the `Recommended` tag for `RAM` in the Hub. Expect the following outcomes:
        - High time to first token (ms) 
        - Low throughput (tokens/sec)

      ### CPU Instruction
      Prebuilt Nitro with llama.cpp inference engine needs to run on the correct CPU instruction.
      Available CPU instruction options are:
      <Tabs items={['AVX2', 'AVX', 'AVX-512']}>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2) for more information.
          - AVX2 is the default instruction set for Intel CPUs and AMD CPUs in Jan.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) for more information.
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX-512](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX-512) for more information.
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
      </Tabs>
      ### Acceleration Options
      Available acceleration options are:
      <Tabs items={['NVIDIA GPU', 'AMD Radeon GPU', 'Intel Arc GPU']}>
        <Tabs.Tab >
          #### Prerequisites
            - Nvidia Driver v535+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
            - CUDA Toolkit v12.2+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
          #### How to Use
            - This feature is supported by `llama.cpp` with CUBLAS build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the NVIDIA GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          #### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          #### How to Use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the AMD Radeon GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          #### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          #### How to Use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the Intel Arc GPU you want.
            - Select a model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub. Expect the following outcomes:
              - High time to first token (ms)
              - Low throughput (tokens/sec)
        </Tabs.Tab>
      </Tabs>
      ## Step-by-step Guide
      1. Navigate to the `App Settings` > `Advanced` > `Open App Directory` > `~/jan/engines` folder.
    </Tabs.Tab>
</Tabs>

2. Modify the `nitro.json` file based on your needs. The default settings are shown below.

```json title="~/jan/engines/nitro.json"
{
  "ctx_len": 2048,
  "ngl": 100,
  "cpu_threads": 1,
  "cont_batching": false,
  "embedding": false
}
```

The table below describes the parameters in the `nitro.json` file.

| Parameter       | Type        | Description                                                                                                                        |
| --------------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| `ctx_len`       | **Integer** | Typically set at `2048`, `ctx_len` provides ample context for model operations like `GPT-3.5`. (_Maximum_: `4096`, _Minimum_: `1`) |
| `ngl`           | **Integer** | Defaulted at `100`, `ngl` determines GPU layer usage.                                                                              |
| `cpu_threads`   | **Integer** | Determines CPU inference threads, limited by hardware and OS. (_Maximum_ determined by system)                                     |
| `cont_batching` | **Integer** | Controls continuous batching, enhancing throughput for LLM inference.                                                              |
| `embedding`     | **Integer** | Enables embedding utilization for tasks like document-enhanced chat in RAG-based applications.                                     |

<Callout emoji="">
  - By default, the value of `ngl` is set to 100, which indicates that it will offload all. If you wish to offload only 50% of the GPU, you can set `ngl` to 15 because most models on Mistral or llama.cpp are around ~ 30 layers.
  - To utilize the embedding feature, include the JSON parameter `"embedding": true`. It will enable Nitro to process inferences with embedding capabilities. Please refer to the [Embedding in the Nitro documentation](https://nitro.jan.ai/features/embed) for a more detailed explanation.
  - To utilize the continuous batching feature for boosting throughput and minimizing latency in large language model (LLM) inference, include `cont_batching: true`. For details, please refer to the [Continuous Batching in the Nitro documentation](https://nitro.jan.ai/features/cont-batch).
</Callout>


<Callout type='info'>
  Assistance and Support, If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.
</Callout>
