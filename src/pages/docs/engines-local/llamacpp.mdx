---
title: LlamaCPP Extension
description: A step-by-step guide on how to customize the LlamaCPP extension.
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    LlamaCPP Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---

import { Tabs } from 'nextra/components'
import { Callout } from 'nextra/components'

# LlamaCPP Extension

## Overview

[Nitro](https://github.com/janhq/nitro) is an inference server on top of [llama.cpp](https://github.com/ggerganov/llama.cpp). It provides an OpenAI-compatible API, queue, & scaling.

## LlamaCPP Extension

<Callout type='info'>
  Nitro is the default AI engine downloaded with Jan. There is no additional setup needed.
</Callout>

In this guide, we'll walk you through the process of customizing your engine settings by configuring the `nitro.json` file

1. Navigate to the `App Settings` > `Advanced` > `Open App Directory` > `~/jan/engine` folder.

<Tabs items={['Mac', 'Windows', 'Linux']}>
    <Tabs.Tab >
      <Tabs items={['Mac Intel', 'Mac Silicon']}>
        <Tabs.Tab >
          Listed here in the Wikipedia page [Intel-based Macs](https://en.wikipedia.org/wiki/MacBook_Pro_(Intel-based))
          
          This uses CPU by default and there is no acceleration option available. 
          
          Please use small models, expect
           - High time to first token (ms) 
           - Low throughput (tokens/sec)
        </Tabs.Tab>

        <Tabs.Tab >
          Listed here in the Wikipedia page [Apple Silicon](https://en.wikipedia.org/wiki/Apple_Silicon)
          
          This can use Apple GPU with Metal by default for acceleration. Apple ANE is not supported yet.

          Please use an adequate model size based on your hardware as it has limited unified memory
          - Low time to first token (ms)
          - High throughput (tokens/sec)
        </Tabs.Tab>
        
      </Tabs>
      Engine location
        ```bash
        ~/jan/engines
        ```
    </Tabs.Tab>
    <Tabs.Tab >
      Currently, only Windows with x86_64 architecture is supported by default. If you do not have any GPU/ NPU, it will use CPU by default.
      
      Please use an adequate model size based on your hardware based on the `Recommended` tag for `RAM` in the Hub
        - High time to first token (ms) 
        - Low throughput (tokens/sec)

      Prebuilt Nitro with llama.cpp inference engine needs to run on the correct CPU instruction.
      Available CPU instruction options are:
      <Tabs items={['AVX2', 'AVX', 'AVX-512']}>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2) for more information
          - AVX2 is the default instruction set for Intel CPUs and AMD CPUs in Jan
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) for more information
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX-512](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX-512) for more information
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
      </Tabs>

      Available acceleration options are:
      <Tabs items={['NVIDIA GPU', 'AMD Radeon GPU', 'Intel Arc GPU']}>
        <Tabs.Tab >
          ### Prerequisites
            - Nvidia Driver v535+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
            - CUDA Toolkit v12.2+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
          ### How to use
            - This feature is supported by `llama.cpp` with CUBLAS build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the NVIDIA GPU you want.
            - Please use an adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          ### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          ### How to use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the AMD Radeon GPU you want.
            - Please use adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          ### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          ### How to use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the Intel Arc GPU you want.
            - Please use adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
      </Tabs>
      Engine location
        ```bash
        C:/Users/<your_user_name>/jan/engines
        ```
    </Tabs.Tab>
    <Tabs.Tab >
        Currently, only Linux with x86_64 architecture is supported. . If you do not have any GPU/ NPU, it will use CPU by default.
      
      Please use an adequate model size based on your hardware based on the `Recommended` tag for `RAM` in the Hub
        - High time to first token (ms) 
        - Low throughput (tokens/sec)

      Prebuilt Nitro with llama.cpp inference engine needs to run on the correct CPU instruction.
      Available CPU instruction options are:
      <Tabs items={['AVX2', 'AVX', 'AVX-512']}>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX2](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX2) for more information
          - AVX2 is the default instruction set for Intel CPUs and AMD CPUs in Jan
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) for more information
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
        <Tabs.Tab >
          - Please refer to this [List of CPUs support AVX-512](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX-512) for more information
          - Please refer to this [discussion](https://github.com/janhq/jan/issues/2489) for manual fixes, we will update the installer soon.
        </Tabs.Tab>
      </Tabs>

      Available acceleration options are:
      <Tabs items={['NVIDIA GPU', 'AMD Radeon GPU', 'Intel Arc GPU']}>
        <Tabs.Tab >
          ### Prerequisites
            - Nvidia Driver v535+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
            - CUDA Toolkit v12.2+ (For installation guide, please see [here](/docs/troubleshooting#1-ensure-gpu-mode-requirements))
          ### How to use
            - This feature is supported by `llama.cpp` with CUBLAS build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the NVIDIA GPU you want.
            - Please use adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          ### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          ### How to use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the AMD Radeon GPU you want.
            - Please use adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
        <Tabs.Tab >
          ### Prerequisites
            - You do not need to install any additional drivers or software for AMD Radeon GPU as it's supported by default with Vulkan.
          ### How to use
            - This feature is supported by `llama.cpp` with Vulkan build
            - Go to `Settings` -> `Advanced Settings` -> `Accelerator` -> Enable and choose the Intel Arc GPU you want.
            - Please use adequate to big model size based on your hardware based on the `Recommended` tag for `VRAM` in the Hub
              - High time to first token (ms) 
              - Low throughput (tokens/sec)
        </Tabs.Tab>
      </Tabs>
        Engine location
        ```bash
        ~/jan/engines
        ```
    </Tabs.Tab>
</Tabs>

2. Modify the `nitro.json` file based on your needs. The default settings are shown below.

```json title="~/jan/engines/nitro.json"
{
  "ctx_len": 2048,
  "ngl": 100,
  "cpu_threads": 1,
  "cont_batching": false,
  "embedding": false
}
```

The table below describes the parameters in the `nitro.json` file.

| Parameter       | Type        | Description                                                                                                                        |
| --------------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| `ctx_len`       | **Integer** | Typically set at `2048`, `ctx_len` provides ample context for model operations like `GPT-3.5`. (_Maximum_: `4096`, _Minimum_: `1`) |
| `ngl`           | **Integer** | Defaulted at `100`, `ngl` determines GPU layer usage.                                                                              |
| `cpu_threads`   | **Integer** | Determines CPU inference threads, limited by hardware and OS. (_Maximum_ determined by system)                                     |
| `cont_batching` | **Integer** | Controls continuous batching, enhancing throughput for LLM inference.                                                              |
| `embedding`     | **Integer** | Enables embedding utilization for tasks like document-enhanced chat in RAG-based applications.                                     |

<Callout emoji="">
  - By default, the value of `ngl` is set to 100, which indicates that it will offload all. If you wish to offload only 50% of the GPU, you can set `ngl` to 15 because most models on Mistral or Llama are around ~ 30 layers.
  - To utilize the embedding feature, include the JSON parameter `"embedding": true`. It will enable Nitro to process inferences with embedding capabilities. Please refer to the [Embedding in the Nitro documentation](https://nitro.jan.ai/features/embed) for a more detailed explanation.
  - To utilize the continuous batching feature for boosting throughput and minimizing latency in large language model (LLM) inference, include `cont_batching: true`. For details, please refer to the [Continuous Batching in the Nitro documentation](https://nitro.jan.ai/features/cont-batch).
</Callout>


<Callout type='info'>
  Assistance and Support, If you have questions, please join our [Discord community](https://discord.gg/Dt7MxDyNNZ) for support, updates, and discussions.
</Callout>