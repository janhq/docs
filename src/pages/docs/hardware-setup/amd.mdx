---
title: AMD GPU
description: Metal GPU support on Jan for llama.cpp
keywords:
  [
    Jan,
    Customizable Intelligence, LLM,
    local AI,
    privacy focus,
    free and open source,
    private and offline,
    conversational AI,
    no-subscription fee,
    large language models,
    Llama CPP integration,
    llama.cpp Extension,
    Intel CPU,
    AMD CPU,
    NVIDIA GPU,
    AMD GPU Radeon,
    Apple Silicon,
    Intel Arc GPU,
  ]
---
import { Callout, Steps } from 'nextra/components'

# AMD GPU

Jan is designed to support AMD GPUs on Windows and Linux exclusively through its Desktop app. The default inference engine for local AI, `llama.cpp`, utilizes a Vulkan backend to ensure compatibility with AMD GPUs, enhancing its performance and reliability for these platforms.

## Pre-requisites
- You do not need to install any additional drivers or software for AMD GPU as it's supported by default with Vulkan.
- The following AMD Radeon cards are confirmed to work effectively:

| AMD GPU                |
|-----------------------|
| Radeon RX Vega series |
| Radeon RX 7000 series |
| Radeon RX 6000 series |
| Radeon RX 5000 series |
| Radeon Pro series     |
| Radeon 600 series     |
| Radeon 500 series     |
| Radeon 400 series     |
| Radeon 300 series     |
| Radeon 200 series     |

<Callout type='info'>
You can find more information about AMD GPU as dGPU [here](https://en.wikipedia.org/wiki/Category:AMD_graphics_cards).
</Callout>


## Enable AMD Radeon RX GPU
To enable the use of AMD GPU in Jan app, follow the steps below:
<Steps>
1. Open Jan application.
2. Go to **Settings** -> **Advanced Settings** -> **Accelerator** -> Enable and choose the Radeon RX GPU that you want.
3. Select a model size based on your hardware based on the **Recommended** tag for **VRAM** in the Hub. Expect the following outcomes:
    - High time to first token (ms)
    - Low throughput (tokens/sec)
</Steps>

## WIP
We are actively monitoring and testing several backend supports from AMD for the Jan application before their official release. Currently, one of the significant features under evaluation includes:
  - **Blas Support on HIP-Supported AMD GPUs with RoCM**: This feature is still in the testing phase. For more details, visit the [Blas support documentation](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#hipblas) on GitHub.